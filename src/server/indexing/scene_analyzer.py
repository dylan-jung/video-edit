import base64
import glob
import json
import os
import subprocess
import tempfile
from io import BytesIO
from typing import Dict, List, Optional, Tuple

import cv2
import numpy as np
from PIL import Image
from sklearn.metrics import silhouette_score
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler
from st_dbscan import ST_DBSCAN

try:
    from kneed import KneeLocator
except ImportError:
    KneeLocator = None  # kneed not installed

from src.server.indexing.clip_embeddings import CLIPMultimodalEmbeddings
from src.server.indexing.vector_db import FrameVectorDB
from src.server.utils.cache_manager import get_cache_path


class SceneAnalyzer:
    """
    ST-DBSCANì„ ì‚¬ìš©í•œ ì‹œê³µê°„ scene ë¶„ì„ í´ë˜ìŠ¤
    1. ë¹„ë””ì˜¤ë¥¼ 1ì´ˆë§ˆë‹¤ ìƒ˜í”Œë§
    2. ê° í”„ë ˆì„ì„ CLIP Embeddingsë¡œ ì„ë² ë”©í•˜ì—¬ FAISS ë²¡í„°DBì— ì €ì¥
    3. ST-DBSCAN í´ëŸ¬ìŠ¤í„°ë§ (ê³µê°„ì  ìœ ì‚¬ì„± + ì‹œê°„ì  ì—°ì†ì„±)
    4. í´ëŸ¬ìŠ¤í„° ê¸°ë°˜ scene ë¶„í• 
    """
    
    def __init__(self, vector_db_path: str = None, model_name: str = "ViT-L-14-336", device: str = None,
                 spatial_eps: float = 35.191, temporal_eps: float = 3.0, min_samples: int = 3):
        """
        Initialize the scene analyzer with CLIP Embeddings and FAISS vector DB
        
        Args:
            vector_db_path: Path to FAISS vector database directory
            model_name: CLIP model name (e.g., "ViT-L-14", "ViT-B-32")
            device: torch device string
            spatial_eps: ê³µê°„ì  ê±°ë¦¬ ì„ê³„ê°’ (CLIP embedding space)
            temporal_eps: ì‹œê°„ì  ê±°ë¦¬ ì„ê³„ê°’ (ì´ˆ ë‹¨ìœ„)
            min_samples: í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•˜ê¸° ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        """
        # CLIP Embeddings ì´ˆê¸°í™”
        self.embeddings_generator = CLIPMultimodalEmbeddings(model_name=model_name, device=device)
        self.scaler = StandardScaler()
        
        # ST-DBSCAN íŒŒë¼ë¯¸í„°
        self.spatial_eps = spatial_eps
        self.temporal_eps = temporal_eps
        self.min_samples = min_samples
        
        # FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
        if vector_db_path is None:
            vector_db_path = "projects/faiss_vector_db"
        
        embedding_dim = self.embeddings_generator.get_embedding_dimension()
        self.vector_db = FrameVectorDB(vector_db_path, embedding_dim=embedding_dim)
        
    def extract_frames_per_second(self, video_path: str, output_dir: str = None) -> List[str]:
        """
        ë¹„ë””ì˜¤ë¥¼ 1ì´ˆë§ˆë‹¤ 1ê°œì˜ jpgë¡œ ìƒ˜í”Œë§
        
        Args:
            video_path: ì…ë ¥ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ (Noneì´ë©´ ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
            
        Returns:
            List of frame file paths
        """
        if output_dir is None:
            output_dir = tempfile.mkdtemp()
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ffmpegë¥¼ ì‚¬ìš©í•˜ì—¬ 1ì´ˆë§ˆë‹¤ í”„ë ˆì„ ì¶”ì¶œ
        frame_pattern = os.path.join(output_dir, "frame_%04d.jpg")
        cmd = [
            'ffmpeg', '-y',
            '-i', video_path,
            '-vf', 'fps=1',  # 1 frame per second
            '-q:v', '2',     # High quality
            frame_pattern
        ]
        
        result = subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        if result.returncode != 0:
            raise Exception(f"Error extracting frames from {video_path}")
        
        # ì¶”ì¶œëœ í”„ë ˆì„ íŒŒì¼ë“¤ì„ ì •ë ¬í•˜ì—¬ ë°˜í™˜
        frame_files = sorted(glob.glob(os.path.join(output_dir, "frame_*.jpg")))
        print(f"âœ… Extracted {len(frame_files)} frames from video")
        
        return frame_files
    
    def generate_embeddings(self, frame_paths: List[str], video_path: str = None, 
                          save_to_db: bool = True) -> np.ndarray:
        """
        ê° í”„ë ˆì„ì— ëŒ€í•´ CLIP Embeddings ìƒì„±í•˜ê³  FAISS ë²¡í„°DBì— ì €ì¥
        ë²¡í„°DBì— ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì„ë² ë”©ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì™€ì„œ ì¬ì‚¬ìš©
        
        Args:
            frame_paths: í”„ë ˆì„ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (ë²¡í„°DB ì €ì¥ìš©)
            save_to_db: ë²¡í„°DBì— ì €ì¥í• ì§€ ì—¬ë¶€
            
        Returns:
            numpy array of embeddings (n_frames, embedding_dim)
        """
        expected_frame_count = len(frame_paths)
        
        # ë²¡í„°DBì—ì„œ ê¸°ì¡´ ì„ë² ë”© í™•ì¸
        if video_path and save_to_db:
            existing_embeddings, existing_metadata = self.vector_db.get_video_embeddings_ordered(video_path)
            
            # ê¸°ì¡´ ì„ë² ë”©ì´ ìˆê³  í”„ë ˆì„ ìˆ˜ê°€ ì¼ì¹˜í•˜ë©´ ì¬ì‚¬ìš©
            if len(existing_embeddings) == expected_frame_count:
                print(f"âœ… Found existing embeddings in vector DB for {expected_frame_count} frames")
                print(f"âœ… Reusing embeddings with shape: {existing_embeddings.shape}")
                return existing_embeddings
            elif len(existing_embeddings) > 0:
                print(f"âš ï¸  Found {len(existing_embeddings)} existing embeddings, but expected {expected_frame_count}")
                print(f"ğŸ”„ Clearing existing embeddings and regenerating...")
                self.vector_db.clear_video(video_path)
        
        print(f"ğŸ”„ Generating CLIP embeddings for {len(frame_paths)} frames...")
        
        # CLIP Embeddings ë°°ì¹˜ ìƒì„±
        embeddings = self.embeddings_generator.generate_embeddings_batch(
            frame_paths, 
            task_type="RETRIEVAL_DOCUMENT",
            batch_size=5  # API ì œí•œ ê³ ë ¤
        )
        
        # ë²¡í„°DBì— ì €ì¥
        if save_to_db and video_path:
            print("ğŸ’¾ Saving embeddings to FAISS vector database...")
            
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadata_list = []
            for i, frame_path in enumerate(frame_paths):
                metadata = {
                    'video_path': video_path,
                    'frame_index': i,
                    'timestamp': float(i),  # 1ì´ˆë§ˆë‹¤ ìƒ˜í”Œë§ì´ë¯€ë¡œ ì¸ë±ìŠ¤ê°€ ê³§ ì´ˆ
                    'frame_path': frame_path
                }
                metadata_list.append(metadata)
            
            # ë°°ì¹˜ë¡œ ë²¡í„°DBì— ì¶”ê°€
            self.vector_db.add_embeddings_batch(embeddings, metadata_list)
            self.vector_db.save_db()
        
        embeddings_array = np.array(embeddings)
        print(f"âœ… Generated embeddings with shape: {embeddings_array.shape}")
        
        return embeddings_array
    
    def perform_clustering(self, embeddings: np.ndarray, timestamps: np.ndarray, 
                          n_clusters: int = None) -> Tuple[np.ndarray, Optional[np.ndarray]]:
        """
        ST-DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (ì‹œê³µê°„ ì°¨ì›)
        
        Args:
            embeddings: ì„ë² ë”© ë°°ì—´
            timestamps: íƒ€ì„ìŠ¤íƒ¬í”„ ë°°ì—´ (ì´ˆ ë‹¨ìœ„)
            n_clusters: ì‚¬ìš©ë˜ì§€ ì•ŠìŒ (ST-DBSCANì—ì„œëŠ” ìë™ ê²°ì •)
            
        Returns:
            Tuple of (cluster_labels, None) - ST-DBSCANì—ì„œëŠ” cluster_centersê°€ ì—†ìŒ
        """
        print("ğŸ”„ Performing ST-DBSCAN clustering...")
        
        # ST-DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        cluster_labels = self.st_dbscan_clustering(embeddings, timestamps)
        
        # ìµœì¢… í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì¶œë ¥
        unique_labels = np.unique(cluster_labels)
        n_clusters_final = len(unique_labels) - (1 if -1 in unique_labels else 0)
        n_noise = np.sum(cluster_labels == -1)
        
        print(f"âœ… Final clustering completed with {n_clusters_final} clusters and {n_noise} noise points")
        
        # í´ëŸ¬ìŠ¤í„°ë³„ í”„ë ˆì„ ìˆ˜ ì¶œë ¥
        for cluster_id in unique_labels:
            if cluster_id == -1:
                print(f"  Noise points: {n_noise} frames")
                continue
            count = np.sum(cluster_labels == cluster_id)
            cluster_timestamps = timestamps[cluster_labels == cluster_id]
            time_span = cluster_timestamps.max() - cluster_timestamps.min() if count > 1 else 0
            print(f"  Cluster {cluster_id}: {count} frames, time span: {time_span:.1f}s")
        
        return cluster_labels, None
    
    def generate_scenes_from_clusters(self, cluster_labels: np.ndarray, 
                                    frame_indices: np.ndarray = None) -> List[Dict]:
        """
        í´ëŸ¬ìŠ¤í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Sceneì˜ start_time, end_time ì„¤ê³„
        ë…¸ì´ì¦ˆ í¬ì¸íŠ¸(-1)ëŠ” ì œì™¸í•˜ê³  ì²˜ë¦¬
        
        Args:
            cluster_labels: í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ë°°ì—´
            frame_indices: í”„ë ˆì„ ì¸ë±ìŠ¤ (outlier ì œê±° í›„)
            
        Returns:
            List of scene dictionaries with start_time and end_time
        """
        if frame_indices is None:
            frame_indices = np.arange(len(cluster_labels))
        
        print("ğŸ”„ Generating scenes from clusters (excluding noise points)...")
        
        # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸(-1) ì œì™¸
        valid_indices = cluster_labels != -1
        valid_cluster_labels = cluster_labels[valid_indices]
        valid_frame_indices = frame_indices[valid_indices]
        
        if len(valid_cluster_labels) == 0:
            print("âš ï¸  All points are noise, no scenes generated")
            return []
        
        scenes = []
        
        # ì—°ì†ëœ ê°™ì€ í´ëŸ¬ìŠ¤í„°ë¥¼ í•˜ë‚˜ì˜ sceneìœ¼ë¡œ ê·¸ë£¹í™”
        current_cluster = valid_cluster_labels[0]
        scene_start = valid_frame_indices[0]
        
        for i in range(1, len(valid_cluster_labels)):
            if valid_cluster_labels[i] != current_cluster:
                # ì´ì „ scene ì¢…ë£Œ
                scene_end = valid_frame_indices[i - 1]
                scenes.append({
                    "start_time": float(scene_start),  # ì´ˆ ë‹¨ìœ„
                    "end_time": float(scene_end + 1),  # ë‹¤ìŒ ì´ˆê¹Œì§€ í¬í•¨
                    "cluster_id": int(current_cluster)
                })
                
                # ìƒˆë¡œìš´ scene ì‹œì‘
                current_cluster = valid_cluster_labels[i]
                scene_start = valid_frame_indices[i]
        
        # ë§ˆì§€ë§‰ scene ì¶”ê°€
        scene_end = valid_frame_indices[-1]
        scenes.append({
            "start_time": float(scene_start),
            "end_time": float(scene_end + 1),
            "cluster_id": int(current_cluster)
        })
        
        print(f"âœ… Generated {len(scenes)} scenes from clusters (noise points excluded)")
        
        # ìƒì„±ëœ scene ì •ë³´ ì¶œë ¥
        for i, scene in enumerate(scenes, 1):
            duration = scene['end_time'] - scene['start_time']
            print(f"  Scene {i}: {int(scene['start_time'] / 60)}m {int(scene['start_time'] % 60)}s - {int(scene['end_time'] / 60)}m {int(scene['end_time'] % 60)}s "
                  f"(duration: {duration:.1f}s, cluster: {scene['cluster_id']})")
        
        return scenes
    
    def search_similar_frames(self, query_embedding: np.ndarray, top_k: int = 5, 
                             threshold: float = 0.7, video_path: str = None) -> List[Tuple[Dict, float]]:
        """
        ìœ ì‚¬í•œ í”„ë ˆì„ ê²€ìƒ‰
        
        Args:
            query_embedding: ì¿¼ë¦¬ ì„ë² ë”©
            top_k: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ìˆ˜
            threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
            video_path: íŠ¹ì • ë¹„ë””ì˜¤ì—ì„œë§Œ ê²€ìƒ‰
            
        Returns:
            List of (metadata, similarity_score) tuples
        """
        return self.vector_db.search_similar(query_embedding, top_k, threshold, video_path)
    
    def analyze_video_scenes(self, video_path: str, use_cache: bool = True) -> List[Dict]:
        """
        ì „ì²´ ë¹„ë””ì˜¤ scene ë¶„ì„ íŒŒì´í”„ë¼ì¸ (ST-DBSCAN ì‚¬ìš©)
        
        Args:
            video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            List of scene dictionaries
        """
        # ìºì‹œ í™•ì¸
        cache_key = {"method": "st_dbscan_clustering", "video_path": video_path, 
                    "spatial_eps": self.spatial_eps, "temporal_eps": self.temporal_eps, 
                    "min_samples": self.min_samples}
        hit, cache_path = get_cache_path(video_path, cache_key)
        
        if hit and use_cache:
            print(f"ğŸ” Using cached scene analysis: {cache_path}")
            with open(cache_path, 'r') as f:
                return json.load(f)
        
        print(f"ğŸ¬ Starting ST-DBSCAN scene analysis for: {video_path}")
        print(f"  Parameters: spatial_eps={self.spatial_eps}, temporal_eps={self.temporal_eps}, min_samples={self.min_samples}")
        
        # 1. ë¹„ë””ì˜¤ì—ì„œ 1ì´ˆë§ˆë‹¤ í”„ë ˆì„ ì¶”ì¶œ
        with tempfile.TemporaryDirectory() as temp_dir:
            frame_paths = self.extract_frames_per_second(video_path, temp_dir)
            
            if len(frame_paths) == 0:
                raise Exception("No frames extracted from video")
            
            # 2. ê° í”„ë ˆì„ì— ëŒ€í•´ CLIP Embeddings ìƒì„±í•˜ê³  FAISS ë²¡í„°DBì— ì €ì¥
            embeddings = self.generate_embeddings(frame_paths, video_path, save_to_db=True)
            
            # 3. íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (1ì´ˆë§ˆë‹¤ ìƒ˜í”Œë§ì´ë¯€ë¡œ ì¸ë±ìŠ¤ê°€ ê³§ ì´ˆ)
            timestamps = np.arange(len(embeddings), dtype=float)
            
            # 4. ST-DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
            cluster_labels, _ = self.perform_clustering(embeddings, timestamps)
            
            # 5. í´ëŸ¬ìŠ¤í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ scene ìƒì„±
            scenes = self.generate_scenes_from_clusters(cluster_labels, timestamps.astype(int))
        
        # ìºì‹œì— ì €ì¥
        if use_cache:
            os.makedirs(os.path.dirname(cache_path), exist_ok=True)
            with open(cache_path, 'w') as f:
                json.dump(scenes, f, indent=2)
        
        print(f"âœ… ST-DBSCAN scene analysis completed: {len(scenes)} scenes generated")
        
        return scenes
    
    def st_dbscan_clustering(self, embeddings: np.ndarray, timestamps: np.ndarray) -> np.ndarray:
        """
        ST-DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰ (ê³µê°„ì  + ì‹œê°„ì  ì°¨ì›)
        
        Args:
            embeddings: CLIP ì„ë² ë”© ë°°ì—´ (n_frames, embedding_dim)
            timestamps: í”„ë ˆì„ íƒ€ì„ìŠ¤íƒ¬í”„ ë°°ì—´ (n_frames,) - ì´ˆ ë‹¨ìœ„
            
        Returns:
            cluster_labels: í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ë°°ì—´ (-1ì€ ë…¸ì´ì¦ˆ)
        """
        print("ğŸ”„ Performing ST-DBSCAN clustering...")
        
        # ì„ë² ë”© ì •ê·œí™”
        embeddings_scaled = self.scaler.fit_transform(embeddings)
        
        print(f"  Spatial features shape: {embeddings_scaled.shape}")
        print(f"  Temporal features shape: {timestamps.shape}")
        print(f"  Parameters: spatial_eps={self.spatial_eps}, temporal_eps={self.temporal_eps}, min_samples={self.min_samples}")

        # --- kâ€‘distance elbow â†’ knee for spatial Îµâ‚› suggestion ---
        try:
            k_val = max(self.min_samples, 2)  # at least 2â€‘NN
            nbrs = NearestNeighbors(n_neighbors=k_val, metric='euclidean').fit(embeddings_scaled)
            dists, _ = nbrs.kneighbors(embeddings_scaled)
            k_dists = np.sort(dists[:, -1])

            if KneeLocator is not None:
                knee = KneeLocator(range(len(k_dists)), k_dists,
                                   curve='convex', direction='increasing')
                if knee.knee is not None:
                    knee_eps = float(k_dists[knee.knee])
                    print(f"ğŸ” kâ€‘distance elbow knee detected â†’ Îµâ‚›â‰ˆ{knee_eps:.3f} "
                          f"(k={k_val}, suggest try ~{knee_eps*1.2:.3f})")
                else:
                    print("âš ï¸  KneeLocator could not find an elbow; inspect kâ€‘distance plot manually.")
            else:
                print("âš ï¸  `kneed` not installed â€“ skipping automatic knee detection.")
        except Exception as e:
            print(f"âš ï¸  Error during kâ€‘distance knee calculation: {e}")
        # ----------------------------------------------------------
        
        # ST-DBSCAN í´ëŸ¬ìŠ¤í„°ë§ ìˆ˜í–‰
        # st_dbscan ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” ê³µê°„ì , ì‹œê°„ì  íŒŒë¼ë¯¸í„°ë¥¼ ë¶„ë¦¬í•˜ì—¬ ì²˜ë¦¬
        st_dbscan = ST_DBSCAN(
            eps1=self.spatial_eps,      # ê³µê°„ì  ê±°ë¦¬ ì„ê³„ê°’
            eps2=self.temporal_eps,     # ì‹œê°„ì  ê±°ë¦¬ ì„ê³„ê°’
            min_samples=self.min_samples
        )
        
        # ë°ì´í„° ì¤€ë¹„: ê³µê°„ ë°ì´í„°ì™€ ì‹œê°„ ë°ì´í„°ë¥¼ ê²°í•©
        # st_dbscanì€ [spatial_features + temporal_features] í˜•íƒœì˜ ë°ì´í„°ë¥¼ ê¸°ëŒ€
        temporal_data = timestamps.reshape(-1, 1)
        combined_data = np.hstack([embeddings_scaled, temporal_data])
        
        with open("combined_data.json", "w") as f:
            f.write(json.dumps(combined_data.tolist()))
            print("âœ… Saved combined data to combined_data.json")
        
        # ST-DBSCAN ì‹¤í–‰
        st_dbscan.fit(combined_data)
        cluster_labels = st_dbscan.labels
        
        # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ë¶„ì„
        unique_labels = np.unique(cluster_labels)
        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)
        n_noise = np.sum(cluster_labels == -1)
        
        print(f"âœ… ST-DBSCAN completed:")
        print(f"  Number of clusters: {n_clusters}")
        print(f"  Number of noise points: {n_noise}")
        print(f"  Noise ratio: {n_noise/len(cluster_labels)*100:.1f}%")
        
        # í´ëŸ¬ìŠ¤í„°ë³„ í”„ë ˆì„ ìˆ˜ ì¶œë ¥
        for label in unique_labels:
            if label == -1:
                continue
            count = np.sum(cluster_labels == label)
            cluster_timestamps = timestamps[cluster_labels == label]
            time_span = cluster_timestamps.max() - cluster_timestamps.min()
            print(f"  Cluster {label}: {count} frames, time span: {time_span:.1f}s")

        # --- Silhouette score for scale sanity check ---
        try:
            if n_clusters > 1:
                sil = silhouette_score(combined_data, cluster_labels, metric='euclidean')
                print(f"ğŸ“ˆ Silhouette score (joint space): {sil:.3f}")
            else:
                print("â„¹ï¸  Silhouette score not computed (only one cluster).")
        except Exception as e:
            print(f"âš ï¸  Error computing silhouette score: {e}")
        # ------------------------------------------------
        
        return cluster_labels


def analyze_video_scenes_clustering(video_path: str, use_cache: bool = True, 
                                   vector_db_path: str = None, model_name: str = "ViT-H-14", device: str = None,
                                   spatial_eps: float = 31.992, temporal_eps: float = 3.0, min_samples: int = 3) -> List[Dict]:
    """
    ST-DBSCAN ê¸°ë°˜ ë¹„ë””ì˜¤ scene ë¶„ì„ í•¨ìˆ˜ (ì‹œê³µê°„ í´ëŸ¬ìŠ¤í„°ë§)
    
    Args:
        video_path: ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€
        vector_db_path: FAISS ë²¡í„° DB ë””ë ‰í† ë¦¬ ê²½ë¡œ
        model_name: CLIP model name (e.g., "ViT-L-14", "ViT-B-32")
        device: torch device string
        spatial_eps: ê³µê°„ì  ê±°ë¦¬ ì„ê³„ê°’ (CLIP embedding space)
        temporal_eps: ì‹œê°„ì  ê±°ë¦¬ ì„ê³„ê°’ (ì´ˆ ë‹¨ìœ„)
        min_samples: í´ëŸ¬ìŠ¤í„°ë¥¼ í˜•ì„±í•˜ê¸° ìœ„í•œ ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        
    Returns:
        List of scene dictionaries with start_time, end_time, cluster_id
    """
    analyzer = SceneAnalyzer(vector_db_path=vector_db_path, model_name=model_name, device=device,
                           spatial_eps=35.3904, temporal_eps=temporal_eps, min_samples=min_samples)
    return analyzer.analyze_video_scenes(video_path, use_cache) 